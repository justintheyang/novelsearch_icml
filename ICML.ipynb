{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv(\"adult.txt\")\n",
    "covtype_df = pd.read_csv(\"covtype.txt\",names=list(range(0,55)))\n",
    "letter_df = pd.read_csv(\"letter-recognition.txt\",names=list(range(0,17))) \n",
    "#census_df = pd.read_csv(\"census-income.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>dat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19121</th>\n",
       "      <td>44</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>56651</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15723</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>130200</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18530</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>310255</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15057</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>242375</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27074</th>\n",
       "      <td>42</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>255847</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17620</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>129150</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>215443</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>70</td>\n",
       "      <td>Private</td>\n",
       "      <td>167358</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19686</th>\n",
       "      <td>27</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>207948</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24438</th>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>112900</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21626</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>75024</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14371</th>\n",
       "      <td>49</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>330874</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26852</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>393480</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31400</th>\n",
       "      <td>50</td>\n",
       "      <td>Private</td>\n",
       "      <td>201882</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>176286</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age          workclass  fnlwgt      education education-num       marital-status          occupation    relationship    race      sex capital-gain capital-loss hours-per-week  native-country  dat\n",
       "19121  44       Self-emp-inc   56651   Some-college            10   Married-civ-spouse     Exec-managerial         Husband   White     Male         7688            0             45   United-States    1\n",
       "15723  36            Private  130200        HS-grad             9             Divorced   Machine-op-inspct       Unmarried   White     Male            0            0             40   United-States    0\n",
       "18530  44            Private  310255   Some-college            10   Married-civ-spouse        Craft-repair         Husband   White     Male            0            0             60   United-States    1\n",
       "15057  23            Private  242375        HS-grad             9        Never-married       Other-service   Not-in-family   White   Female            0            0             30   United-States    0\n",
       "27074  42          Local-gov  255847        HS-grad             9   Married-civ-spouse    Transport-moving         Husband   White     Male            0            0             40   United-States    0\n",
       "17620  36            Private  129150   Some-college            10   Married-civ-spouse      Prof-specialty         Husband   White     Male            0            0             20   United-States    1\n",
       "3340   24            Private  215443        HS-grad             9            Separated       Other-service   Not-in-family   White     Male            0            0             40   United-States    0\n",
       "656    70            Private  167358            9th             5              Widowed      Prof-specialty       Unmarried   White   Female         1111            0             15   United-States    0\n",
       "19686  27   Self-emp-not-inc  207948   Some-college            10        Never-married       Other-service   Not-in-family   Black     Male            0            0             40   United-States    0\n",
       "24438  33            Private  112900        HS-grad             9             Divorced       Other-service       Unmarried   White   Female            0            0             40   United-States    0\n",
       "21626  38            Private   75024        7th-8th             4             Divorced      Prof-specialty   Not-in-family   White   Female            0            0             40   United-States    0\n",
       "14371  49       Self-emp-inc  330874   Some-college            10   Married-civ-spouse               Sales         Husband   White     Male            0            0             70   United-States    1\n",
       "26852  39            Private  393480        HS-grad             9            Separated   Handlers-cleaners   Not-in-family   Black     Male            0            0             40   United-States    0\n",
       "31400  50            Private  201882        Masters            14   Married-civ-spouse   Machine-op-inspct         Husband   White     Male            0            0             30   United-States    0\n",
       "30472  40            Private  176286        HS-grad             9   Married-civ-spouse        Craft-repair         Husband   White     Male            0            0             40   United-States    0"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mean = SimpleImputer(missing_values=\" ?\", strategy=\"most_frequent\").fit(adult_df)\n",
    "adult_df = pd.DataFrame(imp_mean.transform(adult_df), columns=adult_df.columns)\n",
    "\n",
    "# convert prediction data into binary labels\n",
    "adult_df['dat'] = np.where(adult_df['dat'] == ' >50K',1,0)\n",
    "adult_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_column = covtype_df[54]\n",
    "positive_label = pred_column.value_counts().index[0]\n",
    "\n",
    "# convert prediction data into binary labels\n",
    "pred_column = np.where(pred_column == positive_label, 1, 0)\n",
    "\n",
    "covtype_df.drop(columns=[54],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129222</th>\n",
       "      <td>0.360180</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.150322</td>\n",
       "      <td>0.192506</td>\n",
       "      <td>0.168329</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>0.799213</td>\n",
       "      <td>0.405512</td>\n",
       "      <td>0.054371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516196</th>\n",
       "      <td>0.596798</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.042949</td>\n",
       "      <td>0.253230</td>\n",
       "      <td>0.373612</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.214415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528722</th>\n",
       "      <td>0.667334</td>\n",
       "      <td>0.519444</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.191214</td>\n",
       "      <td>0.119854</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.980315</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.291649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474551</th>\n",
       "      <td>0.622811</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>0.251938</td>\n",
       "      <td>0.083041</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.225986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387371</th>\n",
       "      <td>0.742371</td>\n",
       "      <td>0.469444</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.125268</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.414079</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.086993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294730</th>\n",
       "      <td>0.588794</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.303508</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.088942</td>\n",
       "      <td>0.972441</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.279528</td>\n",
       "      <td>0.231842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143713</th>\n",
       "      <td>0.552276</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.173228</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.517634</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.433071</td>\n",
       "      <td>0.288164</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546575</th>\n",
       "      <td>0.730865</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.279885</td>\n",
       "      <td>0.311370</td>\n",
       "      <td>0.649290</td>\n",
       "      <td>0.814961</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.618110</td>\n",
       "      <td>0.130211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328923</th>\n",
       "      <td>0.262631</td>\n",
       "      <td>0.413889</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223514</td>\n",
       "      <td>0.073627</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.397638</td>\n",
       "      <td>0.179841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134430</th>\n",
       "      <td>0.522761</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.030064</td>\n",
       "      <td>0.242894</td>\n",
       "      <td>0.796965</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.413386</td>\n",
       "      <td>0.157396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6         7         8         9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53  54\n",
       "129222  0.360180  0.150000  0.257576  0.150322  0.192506  0.168329  0.897638  0.799213  0.405512  0.054371  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0\n",
       "516196  0.596798  0.022222  0.287879  0.042949  0.253230  0.373612  0.763780  0.779528  0.539370  0.214415  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0\n",
       "528722  0.667334  0.519444  0.212121  0.506800  0.191214  0.119854  0.870079  0.980315  0.622047  0.291649  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1\n",
       "474551  0.622811  0.041667  0.106061  0.151754  0.251938  0.083041  0.842520  0.885827  0.582677  0.225986  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1\n",
       "387371  0.742371  0.469444  0.151515  0.125268  0.197674  0.414079  0.897638  0.960630  0.582677  0.086993  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   0\n",
       "294730  0.588794  0.247222  0.333333  0.303508  0.135659  0.088942  0.972441  0.783465  0.279528  0.231842  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1\n",
       "143713  0.552276  0.263889  0.196970  0.173228  0.279070  0.517634  0.944882  0.870079  0.433071  0.288164  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1\n",
       "546575  0.730865  0.986111  0.121212  0.279885  0.311370  0.649290  0.814961  0.885827  0.618110  0.130211  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   0\n",
       "328923  0.262631  0.413889  0.378788  0.000000  0.223514  0.073627  0.956693  0.905512  0.397638  0.179841  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0\n",
       "134430  0.522761  0.116667  0.287879  0.030064  0.242894  0.796965  0.866142  0.771654  0.413386  0.157396  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(covtype_df)\n",
    "covtype_df = pd.DataFrame(scaler.transform(covtype_df),columns=covtype_df.columns)\n",
    "covtype_df.insert(54,54,pred_column,True)\n",
    "covtype_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_col,pred_column = letter_df[0], letter_df[16]\n",
    "letter_df.drop(columns=[0,16],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12578</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13621</th>\n",
       "      <td>C</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586</th>\n",
       "      <td>E</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>A</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16327</th>\n",
       "      <td>N</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14067</th>\n",
       "      <td>T</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19172</th>\n",
       "      <td>R</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16323</th>\n",
       "      <td>X</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>R</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>N</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15  16  17  18\n",
       "12578  Y  0.466667  0.600000  0.466667  0.466667  0.266667  0.266667  0.600000  0.133333  0.466667  0.666667  0.733333  0.400000  0.133333  0.800000  0.200000   4   0   0\n",
       "13621  C  0.200000  0.333333  0.266667  0.200000  0.133333  0.400000  0.533333  0.466667  0.533333  0.533333  0.533333  0.866667  0.066667  0.600000  0.266667  10   0   1\n",
       "19586  E  0.133333  0.066667  0.133333  0.200000  0.133333  0.466667  0.466667  0.333333  0.400000  0.466667  0.400000  0.600000  0.133333  0.533333  0.333333  10   0   1\n",
       "2500   A  0.133333  0.200000  0.200000  0.133333  0.066667  0.666667  0.133333  0.133333  0.066667  0.533333  0.133333  0.600000  0.066667  0.400000  0.066667   8   0   1\n",
       "16327  N  0.200000  0.400000  0.266667  0.266667  0.133333  0.466667  0.466667  0.933333  0.133333  0.333333  0.400000  0.533333  0.400000  0.533333  0.000000   8   0   0\n",
       "14067  T  0.266667  0.466667  0.400000  0.333333  0.400000  0.466667  0.466667  0.266667  0.333333  0.466667  0.400000  0.600000  0.333333  0.533333  0.333333   7   0   0\n",
       "19172  R  0.266667  0.533333  0.333333  0.400000  0.400000  0.466667  0.466667  0.466667  0.200000  0.533333  0.333333  0.466667  0.266667  0.466667  0.466667  10   0   0\n",
       "16323  X  0.333333  0.666667  0.400000  0.533333  0.266667  0.466667  0.466667  0.266667  0.266667  0.466667  0.400000  0.533333  0.200000  0.533333  0.266667   8   0   0\n",
       "14171  R  0.200000  0.533333  0.266667  0.400000  0.266667  0.333333  0.733333  0.466667  0.200000  0.533333  0.266667  0.600000  0.133333  0.400000  0.333333  11   0   0\n",
       "13800  N  0.266667  0.266667  0.333333  0.400000  0.133333  0.466667  0.466667  0.933333  0.133333  0.266667  0.400000  0.533333  0.400000  0.533333  0.000000   8   0   0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(letter_df)\n",
    "letter_df = pd.DataFrame(scaler.transform(letter_df),columns=letter_df.columns)\n",
    "letter_df.insert(0,0,letter_col,True)\n",
    "letter_df.insert(16,16,pred_column,True)\n",
    "\n",
    "# two ways to make binary classification labels\n",
    "letter_df_p1 = np.where(letter_df[0] == 'O', 1, 0)\n",
    "letter_df_p2 = np.where(letter_df[0].isin([chr(x) for x in range(ord('A'), ord('M') + 1)]), 1, 0)\n",
    "letter_df.insert(17,17,letter_df_p1,True)\n",
    "letter_df.insert(18,18,letter_df_p2,True)\n",
    "\n",
    "\n",
    "letter_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df.replace([' Not in universe', ' ?'], np.nan, inplace = True) \n",
    "census_df.dropna(axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "search_space = [{'classifier': [LogisticRegression(solver='saga')],\n",
    "                 'classifier__penalty': ['none','l1','l2'],\n",
    "                 'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 100, 1000],\n",
    "                 'classifier__max_features': [1,2,6,12, 20]}]\n",
    "# Create grid search \n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), verbose=0)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(covtype_df.iloc[:5000,:-1], covtype_df.iloc[:5000,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = covtype_df.iloc[:10000,:-1], covtype_df.iloc[:10000,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# take all our penguin data, and reserve 50% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(solver='saga',\n",
    "                          random_state=0)\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = SVC(random_state=0)\n",
    "\n",
    "clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "clf5 = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "# clf6 = OrthogonalMatchingPursuit()\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('classifier', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('classifier', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('classifier', clf5)])\n",
    "\n",
    "# pipe6 = Pipeline([('classifier', clf6)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__penalty': ['none', 'l1', 'l2'],\n",
    "                'classifier__C': np.logspace(-8, 4, 13)}]\n",
    "\n",
    "param_grid2 = [{'classifier__n_neighbors': np.arange(1, 100, 10)}]\n",
    "#                 'classifier__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'classifier__kernel': ['rbf'],\n",
    "                'classifier__C': np.power(10., np.arange(-4, 4)),\n",
    "                'classifier__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'classifier__kernel': ['linear'],                \n",
    "                'classifier__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid4 = [{'classifier__n_estimators': [1024],\n",
    "                'classifier__max_features': [1,2,4,6,8,12,16,20]}]\n",
    "\n",
    "param_grid5 = [{'classifier__C': np.logspace(-8,4,13),\n",
    "               'classifier__loss': ['hinge', 'squared_hinge']}]\n",
    "\n",
    "# param_grid6 = [{'classifier__n_nonzero_coefs': [.01, .05, .1, .2, .6]}]\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                            (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                            ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=1,\n",
    "                       cv=2, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "?accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 88.85% | outer ACC 88.70%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 87.78% | outer ACC 86.80%\n",
      "outer fold 1/5 | tuning PAC      | inner ACC 87.15% | outer ACC 86.50%\n",
      "outer fold 1/5 | tuning RF       | inner ACC 90.62% | outer ACC 90.70%\n",
      "outer fold 1/5 | tuning SVM      | inner ACC 88.70% | outer ACC 88.20%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 88.15% | outer ACC 88.90%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 87.48% | outer ACC 87.40%\n",
      "outer fold 2/5 | tuning PAC      | inner ACC 87.13% | outer ACC 87.30%\n",
      "outer fold 2/5 | tuning RF       | inner ACC 90.72% | outer ACC 90.70%\n",
      "outer fold 2/5 | tuning SVM      | inner ACC 88.95% | outer ACC 89.30%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 88.22% | outer ACC 88.80%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 87.28% | outer ACC 88.00%\n",
      "outer fold 3/5 | tuning PAC      | inner ACC 87.05% | outer ACC 87.30%\n",
      "outer fold 3/5 | tuning RF       | inner ACC 89.85% | outer ACC 92.10%\n",
      "outer fold 3/5 | tuning SVM      | inner ACC 88.40% | outer ACC 88.70%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 88.15% | outer ACC 90.00%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 87.40% | outer ACC 87.20%\n",
      "outer fold 4/5 | tuning PAC      | inner ACC 87.20% | outer ACC 87.10%\n",
      "outer fold 4/5 | tuning RF       | inner ACC 90.40% | outer ACC 90.90%\n",
      "outer fold 4/5 | tuning SVM      | inner ACC 88.42% | outer ACC 89.10%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 88.38% | outer ACC 89.20%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 87.40% | outer ACC 87.60%\n",
      "outer fold 5/5 | tuning PAC      | inner ACC 87.33% | outer ACC 86.80%\n",
      "outer fold 5/5 | tuning RF       | inner ACC 90.62% | outer ACC 90.60%\n",
      "outer fold 5/5 | tuning SVM      | inner ACC 88.83% | outer ACC 89.40%\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the \n",
    "    # penguin data is in a Dataframe instead of a numpy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic | outer CV acc. 87.40% +\\- 0.400\n",
      "KNN      | outer CV acc. 89.12% +\\- 0.471\n",
      "SVM      | outer CV acc. 88.94% +\\- 0.441\n",
      "RF       | outer CV acc. 91.00% +\\- 0.559\n",
      "PAC      | outer CV acc. 87.00% +\\- 0.310\n",
      "\n",
      "Logistic best parameters {'classifier__C': 100.0, 'classifier__penalty': 'l1'}\n",
      "KNN best parameters {'classifier__n_neighbors': 11}\n",
      "SVM best parameters {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "RF best parameters {'classifier__max_features': 20, 'classifier__n_estimators': 1024}\n",
      "PAC best parameters {'classifier__C': 0.1, 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 88.62% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['SVM'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = letter_df.iloc[:10000,1:-2], letter_df.iloc[:10000,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# take all our penguin data, and reserve 50% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(solver='saga',\n",
    "                          random_state=0)\n",
    "\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = SVC(random_state=0)\n",
    "\n",
    "clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "clf5 = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "# clf6 = OrthogonalMatchingPursuit()\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('classifier', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('classifier', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('classifier', clf5)])\n",
    "\n",
    "# pipe6 = Pipeline([('classifier', clf6)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__penalty': ['none', 'l1', 'l2'],\n",
    "                'classifier__C': np.logspace(-8, 4, 13)}]\n",
    "\n",
    "param_grid2 = [{'classifier__n_neighbors': np.arange(1, 100, 10)}]\n",
    "#                 'classifier__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'classifier__kernel': ['rbf'],\n",
    "                'classifier__C': np.power(10., np.arange(-4, 4)),\n",
    "                'classifier__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'classifier__kernel': ['linear'],                \n",
    "                'classifier__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid4 = [{'classifier__n_estimators': [1024],\n",
    "                'classifier__max_features': [1,2,4,6,8,12,16,20]}]\n",
    "\n",
    "param_grid5 = [{'classifier__C': np.logspace(-8,4,13),\n",
    "               'classifier__loss': ['hinge', 'squared_hinge']}]\n",
    "\n",
    "# param_grid6 = [{'classifier__n_nonzero_coefs': [.01, .05, .1, .2, .6]}]\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                            (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                            ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=1,\n",
    "                       cv=2, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 97.78% | outer ACC 98.60%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 1/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 1/5 | tuning RF       | inner ACC 98.38% | outer ACC 98.90%\n",
      "outer fold 1/5 | tuning SVM      | inner ACC 97.97% | outer ACC 98.60%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 98.23% | outer ACC 98.60%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 2/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 2/5 | tuning RF       | inner ACC 98.40% | outer ACC 98.50%\n",
      "outer fold 2/5 | tuning SVM      | inner ACC 98.50% | outer ACC 98.10%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 97.85% | outer ACC 98.30%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 3/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 3/5 | tuning RF       | inner ACC 98.38% | outer ACC 98.30%\n",
      "outer fold 3/5 | tuning SVM      | inner ACC 98.12% | outer ACC 98.70%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 97.88% | outer ACC 98.80%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 4/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 4/5 | tuning RF       | inner ACC 98.23% | outer ACC 98.70%\n",
      "outer fold 4/5 | tuning SVM      | inner ACC 98.20% | outer ACC 98.50%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 97.88% | outer ACC 99.10%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 5/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 5/5 | tuning RF       | inner ACC 98.28% | outer ACC 98.40%\n",
      "outer fold 5/5 | tuning SVM      | inner ACC 98.12% | outer ACC 98.50%\n",
      "Wall time: 8h 45min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the \n",
    "    # penguin data is in a Dataframe instead of a numpy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic | outer CV acc. 96.20% +\\- 0.000\n",
      "KNN      | outer CV acc. 98.68% +\\- 0.264\n",
      "SVM      | outer CV acc. 98.48% +\\- 0.204\n",
      "RF       | outer CV acc. 98.56% +\\- 0.215\n",
      "PAC      | outer CV acc. 96.20% +\\- 0.000\n",
      "\n",
      "Logistic best parameters {'classifier__C': 1e-08, 'classifier__penalty': 'none'}\n",
      "KNN best parameters {'classifier__n_neighbors': 1}\n",
      "SVM best parameters {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "RF best parameters {'classifier__max_features': 6, 'classifier__n_estimators': 1024}\n",
      "PAC best parameters {'classifier__C': 1e-08, 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 98.14% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 98.58%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['SVM'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00999248, 0.00897574, 0.00997722, 0.00846028, 0.01097262,\n",
       "        0.01098752, 0.00797904, 0.00797808, 0.01346684, 0.00997305]),\n",
       " 'std_fit_time': array([1.98352337e-03, 9.96589661e-04, 1.54972076e-06, 5.14507294e-04,\n",
       "        1.99687481e-03, 9.82999802e-04, 9.96708870e-04, 1.19209290e-07,\n",
       "        3.46350670e-03, 9.97543335e-04]),\n",
       " 'mean_score_time': array([0.16227698, 0.26413703, 0.28124559, 0.25383055, 0.24334741,\n",
       "        0.2228924 , 0.30595279, 0.27277231, 0.36454248, 0.28488588]),\n",
       " 'std_score_time': array([0.0022912 , 0.00365424, 0.04188406, 0.01446545, 0.01396132,\n",
       "        0.00147712, 0.05162477, 0.00349283, 0.03889847, 0.01658368]),\n",
       " 'param_classifier__n_neighbors': masked_array(data=[1, 11, 21, 31, 41, 51, 61, 71, 81, 91],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__n_neighbors': 1},\n",
       "  {'classifier__n_neighbors': 11},\n",
       "  {'classifier__n_neighbors': 21},\n",
       "  {'classifier__n_neighbors': 31},\n",
       "  {'classifier__n_neighbors': 41},\n",
       "  {'classifier__n_neighbors': 51},\n",
       "  {'classifier__n_neighbors': 61},\n",
       "  {'classifier__n_neighbors': 71},\n",
       "  {'classifier__n_neighbors': 81},\n",
       "  {'classifier__n_neighbors': 91}],\n",
       " 'split0_test_score': array([0.982 , 0.9796, 0.9772, 0.9768, 0.9724, 0.9684, 0.9656, 0.9628,\n",
       "        0.9624, 0.962 ]),\n",
       " 'split1_test_score': array([0.9808, 0.9756, 0.9784, 0.9732, 0.9712, 0.9692, 0.964 , 0.962 ,\n",
       "        0.962 , 0.962 ]),\n",
       " 'mean_test_score': array([0.9814, 0.9776, 0.9778, 0.975 , 0.9718, 0.9688, 0.9648, 0.9624,\n",
       "        0.9622, 0.962 ]),\n",
       " 'std_test_score': array([0.0006, 0.002 , 0.0006, 0.0018, 0.0006, 0.0004, 0.0008, 0.0004,\n",
       "        0.0002, 0.    ]),\n",
       " 'rank_test_score': array([ 1,  3,  2,  4,  5,  6,  7,  8,  9, 10])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_algo.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([covtype_df.iloc[5000:,:-1], covtype_df.iloc[5000:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = best_model.best_params_['classifier'].fit(covtype_df.iloc[:5000,:-1], covtype_df.iloc[:5000,-1]).predict(covtype_df.iloc[5000:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-38401116fb0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovtype_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovtype_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-700440aa295b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovtype_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters = {'C':[1],'penalty':['l2']}\n",
    "X = covtype_df.drop(columns=[0])\n",
    "y = np.where(covtype_df[16] == 2,1,0)\n",
    "log_reg = LogisticRegression()\n",
    "clf = GridSearchCV(estimator=log_reg,param_grid=parameters)\n",
    "clf.fit(X,y)\n",
    "print(clf.score(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = adult_df[['workclass', 'marital-status', 'occupation', 'relationship']]\n",
    "y = np.where(adult_df.iloc[:,-1] == ' >50K',1,0)\n",
    "\n",
    "column_trans = make_column_transformer((OneHotEncoder(handle_unknown='ignore'),\n",
    "                                        ['workclass', 'marital-status', 'occupation']),\n",
    "                                      (OrdinalEncoder(), ['relationship']),\n",
    "                                      remainder='drop')\n",
    "logreg = LogisticRegression()\n",
    "pipe = make_pipeline(column_trans, logreg)\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range 3: \n",
    "    run_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(data):\n",
    "    iterations = 3\n",
    "    for i in range(iterations):\n",
    "        # draw 5k samples for training data, and set aside the rest for testing \n",
    "        X_train, Y_train, X_test, Y_test = draw_samples(data)\n",
    "        # returns the gridsearchCV model list thing\n",
    "        gridcvs = create_gridsearch()\n",
    "        \n",
    "        select_best_and_fit()\n",
    "        \n",
    "        get_statistics()\n",
    "        \n",
    "        send_brain_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples(data, n = 5000):\n",
    "    train_index = random.sample(range(0,len(data)), n)\n",
    "    # assumes target column is last column\n",
    "    X_train, Y_train = data.iloc[train_index, :-1], data.iloc[train_index, -1]\n",
    "    \n",
    "    test = data[~data.index.isin(train_index)]\n",
    "    X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170611</th>\n",
       "      <td>0.521761</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.173228</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.169594</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.838583</td>\n",
       "      <td>0.570866</td>\n",
       "      <td>0.226683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518303</th>\n",
       "      <td>0.738369</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.430208</td>\n",
       "      <td>0.397933</td>\n",
       "      <td>0.060419</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.043636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60556</th>\n",
       "      <td>0.569785</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.464567</td>\n",
       "      <td>0.355297</td>\n",
       "      <td>0.840382</td>\n",
       "      <td>0.826772</td>\n",
       "      <td>0.968504</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.890004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213381</th>\n",
       "      <td>0.699350</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.242663</td>\n",
       "      <td>0.262274</td>\n",
       "      <td>0.492483</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.251080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243759</th>\n",
       "      <td>0.577289</td>\n",
       "      <td>0.213889</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.266150</td>\n",
       "      <td>0.160461</td>\n",
       "      <td>0.913386</td>\n",
       "      <td>0.877953</td>\n",
       "      <td>0.480315</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194480</th>\n",
       "      <td>0.679840</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.088762</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.844457</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434244</th>\n",
       "      <td>0.556278</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.042949</td>\n",
       "      <td>0.249354</td>\n",
       "      <td>0.206688</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807087</td>\n",
       "      <td>0.147358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464199</th>\n",
       "      <td>0.645823</td>\n",
       "      <td>0.086111</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.413028</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.238022</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.822835</td>\n",
       "      <td>0.496063</td>\n",
       "      <td>0.363446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455444</th>\n",
       "      <td>0.735368</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.060845</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.496979</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224997</th>\n",
       "      <td>0.578289</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.486042</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.156527</td>\n",
       "      <td>0.948819</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>0.448819</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6         7         8         9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53\n",
       "170611  0.521761  0.022222  0.196970  0.173228  0.193798  0.169594  0.803150  0.838583  0.570866  0.226683  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "518303  0.738369  0.352778  0.333333  0.430208  0.397933  0.060419  0.984252  0.866142  0.338583  0.043636  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "60556   0.569785  0.650000  0.090909  0.464567  0.355297  0.840382  0.826772  0.968504  0.681102  0.890004  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "213381  0.699350  0.583333  0.151515  0.242663  0.262274  0.492483  0.834646  0.984252  0.677165  0.251080  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "243759  0.577289  0.213889  0.136364  0.095920  0.266150  0.160461  0.913386  0.877953  0.480315  0.289000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "194480  0.679840  0.625000  0.060606  0.088762  0.220930  0.844457  0.842520  0.956693  0.653543  0.213300  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "434244  0.556278  0.641667  0.348485  0.042949  0.249354  0.206688  0.700787  1.000000  0.807087  0.147358  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "464199  0.645823  0.086111  0.212121  0.413028  0.395349  0.238022  0.850394  0.822835  0.496063  0.363446  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "455444  0.735368  0.855556  0.287879  0.060845  0.255814  0.496979  0.645669  0.881890  0.783465  0.230029  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "224997  0.578289  0.325000  0.196970  0.486042  0.162791  0.156527  0.948819  0.897638  0.448819  0.167155  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[5000 rows x 54 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = draw_samples(covtype_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridsearch():\n",
    "    scoring = {'accuracy' : 'accuracy',\n",
    "               'f1' : 'f1', \n",
    "               'roc_auc' : 'roc_auc',\n",
    "              }#            'MCC' : make_scorer(matthews_corrcoef)}\n",
    "    \n",
    "    # Initializing Classifiers\n",
    "    clf1 = LogisticRegression(solver='saga',\n",
    "                              random_state=0)\n",
    "\n",
    "    clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                                leaf_size=50)\n",
    "    clf3 = SVC(random_state=0)\n",
    "\n",
    "    clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "    clf5 = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "\n",
    "    # Building the pipelines\n",
    "    pipe1 = Pipeline([('classifier', clf1)])\n",
    "\n",
    "    pipe2 = Pipeline([('classifier', clf2)])\n",
    "\n",
    "    pipe3 = Pipeline([('classifier', clf3)])\n",
    "\n",
    "    pipe4 = Pipeline([('classifier', clf4)])\n",
    "\n",
    "    pipe5 = Pipeline([('classifier', clf5)])\n",
    "\n",
    "\n",
    "\n",
    "    # Setting up the parameter grids\n",
    "    param_grid1 = [{'classifier__penalty': ['none', 'l1', 'l2'],\n",
    "                    'classifier__C': np.logspace(-8, 4, 13)}]\n",
    "\n",
    "    param_grid2 = [{'classifier__n_neighbors': np.geomspace(1, 500, num=25, dtype=int),\n",
    "                    'classifier__weights': ['uniform', 'distance']}]\n",
    "\n",
    "    param_grid3 = [{'classifier__kernel': ['rbf'],\n",
    "                    'classifier__C': np.power(10., np.arange(-7, 4)),\n",
    "                    'classifier__gamma': [0.001,0.005,0.01,0.05,0.1,0.5,1,2]},\n",
    "                   {'classifier__kernel': ['linear'],                \n",
    "                    'classifier__C': np.power(10., np.arange(-7, 4))},\n",
    "                   {'classifier__kernel': ['polynomial'],\n",
    "                    'classifier__degree': [2,3],\n",
    "                    'classifier__C': np.power(10., np.arange(-7, 4))}]\n",
    "\n",
    "    param_grid4 = [{'classifier__n_estimators': [1024],\n",
    "                    'classifier__max_features': [1,2,4,6,8,12,16, 20]}]\n",
    "\n",
    "    param_grid5 = [{'classifier__C': np.logspace(-8,4,13),\n",
    "                    'classifier__loss': ['hinge', 'squared_hinge']}]\n",
    "\n",
    "\n",
    "    # Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "    gridcvs = {}\n",
    "\n",
    "    for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                                (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                                ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):\n",
    "        gcv = GridSearchCV(estimator=est,\n",
    "                           param_grid=pgrid,\n",
    "                           scoring='accuracy', #scoring\n",
    "                           n_jobs=1,\n",
    "                           cv=5, \n",
    "                           verbose=0,\n",
    "                           refit=True)\n",
    "        gridcvs[name] = gcv\n",
    "    return gridcvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                                (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                                ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAC\n"
     ]
    }
   ],
   "source": [
    "gridcvs = create_gridsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appapppoopoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning PAC      | inner ACC 85.40% | outer ACC 85.30%\n",
      "\n",
      "outer fold 2/5 | tuning PAC      | inner ACC 85.38% | outer ACC 85.40%\n",
      "\n",
      "outer fold 3/5 | tuning PAC      | inner ACC 85.38% | outer ACC 85.40%\n",
      "\n",
      "outer fold 4/5 | tuning PAC      | inner ACC 85.38% | outer ACC 85.40%\n",
      "\n",
      "outer fold 5/5 | tuning PAC     "
     ]
    }
   ],
   "source": [
    "appapppoopoo = run_gridsearch(gridcvs, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "    # The inner loop for hyperparameter tuning\n",
    "    gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "    y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "    acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "    print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "          (gs_est.best_score_ * 100, acc * 100))\n",
    "    cv_scores[name].append(acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gridsearch(gridcvs, X_train, Y_train):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "    skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # The outer loop for algorithm selection\n",
    "    c = 1\n",
    "    for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "        for name, gs_est in sorted(gridcvs.items()):\n",
    "            print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "            # The inner loop for hyperparameter tuning\n",
    "            gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "            y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "            acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "            print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "                  (gs_est.best_score_ * 100, acc * 100))\n",
    "            cv_scores[name].append(acc)\n",
    "\n",
    "        c += 1\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('P', GridSearchCV(cv=5, estimator=PassiveAggressiveClassifier(random_state=0),\n",
       "             n_jobs=1,\n",
       "             param_grid={'classifier__C': array([1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01,\n",
       "       1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04]),\n",
       "                         'classifier__loss': ['hinge', 'squared_hinge']},\n",
       "             refit=False, scoring='accuracy'))])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcvs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning P       "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter classifier for estimator PassiveAggressiveClassifier(random_state=0). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mEstimator\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \"\"\"\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    250\u001b[0m                                  \u001b[1;34m'Check the list of available parameters '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                                  \u001b[1;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                                  (key, self))\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter classifier for estimator PassiveAggressiveClassifier(random_state=0). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the \n",
    "    # penguin data is in a Dataframe instead of a numpy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
