{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv(\"adult.txt\")\n",
    "covtype_df = pd.read_csv(\"covtype.txt\",names=list(range(0,55)))\n",
    "letter_df = pd.read_csv(\"letter-recognition.txt\",names=list(range(0,17)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>dat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27311</th>\n",
       "      <td>49</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>168191</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>Italy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7259</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>231263</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19050</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>61272</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11645</th>\n",
       "      <td>18</td>\n",
       "      <td>Private</td>\n",
       "      <td>318082</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9694</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>156266</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Amer-Indian-Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11169</th>\n",
       "      <td>50</td>\n",
       "      <td>Private</td>\n",
       "      <td>202044</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>61</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>113544</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19898</th>\n",
       "      <td>45</td>\n",
       "      <td>Private</td>\n",
       "      <td>252079</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24117</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>282389</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14869</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>214689</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19505</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>119665</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>100506</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>190385</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19013</th>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>379798</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29650</th>\n",
       "      <td>46</td>\n",
       "      <td>Private</td>\n",
       "      <td>197731</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age          workclass  fnlwgt      education education-num  \\\n",
       "27311  49   Self-emp-not-inc  168191        7th-8th             4   \n",
       "7259   30            Private  231263        HS-grad             9   \n",
       "19050  34            Private   61272      Bachelors            13   \n",
       "11645  18            Private  318082   Some-college            10   \n",
       "9694   27            Private  156266        HS-grad             9   \n",
       "11169  50            Private  202044      Bachelors            13   \n",
       "1205   61       Self-emp-inc  113544        Masters            14   \n",
       "19898  45            Private  252079   Some-college            10   \n",
       "24117  28            Private  282389        HS-grad             9   \n",
       "14869  28            Private  214689      Bachelors            13   \n",
       "19505  23            Private  119665        HS-grad             9   \n",
       "4407   52       Self-emp-inc  100506   Some-college            10   \n",
       "21945  34            Private  190385        Masters            14   \n",
       "19013  33            Private  379798        HS-grad             9   \n",
       "29650  46            Private  197731      Assoc-voc            11   \n",
       "\n",
       "               marital-status          occupation     relationship  \\\n",
       "27311      Married-civ-spouse       Other-service          Husband   \n",
       "7259       Married-civ-spouse               Sales          Husband   \n",
       "19050           Never-married     Exec-managerial    Not-in-family   \n",
       "11645           Never-married               Sales        Own-child   \n",
       "9694            Never-married               Sales        Own-child   \n",
       "11169                Divorced      Prof-specialty        Unmarried   \n",
       "1205       Married-civ-spouse               Sales          Husband   \n",
       "19898      Married-civ-spouse      Prof-specialty          Husband   \n",
       "24117           Never-married    Transport-moving    Not-in-family   \n",
       "14869           Never-married   Handlers-cleaners   Other-relative   \n",
       "19505           Never-married        Adm-clerical    Not-in-family   \n",
       "4407       Married-civ-spouse               Sales          Husband   \n",
       "21945           Never-married      Prof-specialty    Not-in-family   \n",
       "19013      Married-civ-spouse        Craft-repair          Husband   \n",
       "29650   Married-spouse-absent   Machine-op-inspct        Unmarried   \n",
       "\n",
       "                      race      sex capital-gain capital-loss hours-per-week  \\\n",
       "27311                White     Male            0            0             70   \n",
       "7259                 White     Male            0            0             40   \n",
       "19050                White   Female            0            0             40   \n",
       "11645                White     Male            0            0             35   \n",
       "9694    Amer-Indian-Eskimo     Male            0            0             20   \n",
       "11169                White   Female            0            0             45   \n",
       "1205                 White     Male            0            0             40   \n",
       "19898                White     Male            0            0             50   \n",
       "24117                White     Male            0            0             60   \n",
       "14869                White     Male            0            0             25   \n",
       "19505                White     Male            0            0             60   \n",
       "4407                 White     Male        15024            0             50   \n",
       "21945                White   Female            0            0             40   \n",
       "19013                White     Male            0            0             40   \n",
       "29650                White     Male            0            0             40   \n",
       "\n",
       "       native-country  dat  \n",
       "27311           Italy    0  \n",
       "7259    United-States    0  \n",
       "19050   United-States    0  \n",
       "11645   United-States    0  \n",
       "9694    United-States    0  \n",
       "11169   United-States    0  \n",
       "1205    United-States    1  \n",
       "19898   United-States    1  \n",
       "24117   United-States    0  \n",
       "14869   United-States    0  \n",
       "19505   United-States    0  \n",
       "4407    United-States    1  \n",
       "21945   United-States    1  \n",
       "19013   United-States    0  \n",
       "29650   United-States    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mean = SimpleImputer(missing_values=\" ?\", strategy=\"most_frequent\").fit(adult_df)\n",
    "adult_df = pd.DataFrame(imp_mean.transform(adult_df), columns=adult_df.columns)\n",
    "\n",
    "# convert prediction data into binary labels\n",
    "adult_df['dat'] = np.where(adult_df['dat'] == ' >50K',1,0)\n",
    "adult_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_column = covtype_df[54]\n",
    "positive_label = pred_column.value_counts().index[0]\n",
    "\n",
    "# convert prediction data into binary labels\n",
    "pred_column = np.where(pred_column == positive_label, 1, 0)\n",
    "\n",
    "covtype_df.drop(columns=[54],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302572</th>\n",
       "      <td>0.561781</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.154617</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.358859</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.236024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151768</th>\n",
       "      <td>0.609805</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.202577</td>\n",
       "      <td>0.217054</td>\n",
       "      <td>0.693832</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.578740</td>\n",
       "      <td>0.151540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313636</th>\n",
       "      <td>0.308154</td>\n",
       "      <td>0.172222</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.042949</td>\n",
       "      <td>0.272610</td>\n",
       "      <td>0.112266</td>\n",
       "      <td>0.901575</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>0.066081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464967</th>\n",
       "      <td>0.613807</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.085898</td>\n",
       "      <td>0.258398</td>\n",
       "      <td>0.242377</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.901575</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.058135</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106241</th>\n",
       "      <td>0.620810</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.231210</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.533511</td>\n",
       "      <td>0.830709</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.519727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43109</th>\n",
       "      <td>0.428714</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.136006</td>\n",
       "      <td>0.262274</td>\n",
       "      <td>0.210763</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.720472</td>\n",
       "      <td>0.530322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462810</th>\n",
       "      <td>0.564782</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.021475</td>\n",
       "      <td>0.236434</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.704724</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.110414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289791</th>\n",
       "      <td>0.217609</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.183250</td>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.162709</td>\n",
       "      <td>0.574803</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.814961</td>\n",
       "      <td>0.113063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173926</th>\n",
       "      <td>0.662331</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>0.280362</td>\n",
       "      <td>0.638331</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.452756</td>\n",
       "      <td>0.305590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233859</th>\n",
       "      <td>0.403202</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.107373</td>\n",
       "      <td>0.253230</td>\n",
       "      <td>0.231980</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>0.311024</td>\n",
       "      <td>0.162415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "302572  0.561781  0.119444  0.166667  0.154617  0.255814  0.358859  0.874016   \n",
       "151768  0.609805  0.088889  0.060606  0.202577  0.217054  0.693832  0.862205   \n",
       "313636  0.308154  0.172222  0.454545  0.042949  0.272610  0.112266  0.901575   \n",
       "464967  0.613807  0.916667  0.136364  0.085898  0.258398  0.242377  0.783465   \n",
       "106241  0.620810  0.050000  0.181818  0.231210  0.232558  0.533511  0.830709   \n",
       "43109   0.428714  0.608333  0.212121  0.136006  0.262274  0.210763  0.803150   \n",
       "462810  0.564782  0.930556  0.257576  0.021475  0.236434  0.040045  0.704724   \n",
       "289791  0.217609  0.855556  0.363636  0.183250  0.329457  0.162709  0.574803   \n",
       "173926  0.662331  0.266667  0.166667  0.394417  0.280362  0.638331  0.937008   \n",
       "233859  0.403202  0.133333  0.378788  0.107373  0.253230  0.231980  0.866142   \n",
       "\n",
       "              7         8         9   ...   45   46   47   48   49   50   51  \\\n",
       "302572  0.850394  0.500000  0.236024  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "151768  0.905512  0.578740  0.151540  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "313636  0.645669  0.181102  0.066081  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "464967  0.901575  0.669291  0.058135  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "106241  0.846457  0.547244  0.519727  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "43109   0.992126  0.720472  0.530322  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "462810  0.846457  0.681102  0.110414  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "289791  0.850394  0.814961  0.113063  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "173926  0.881890  0.452756  0.305590  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "233859  0.700787  0.311024  0.162415  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "         52   53  54  \n",
       "302572  0.0  0.0   0  \n",
       "151768  0.0  0.0   0  \n",
       "313636  0.0  0.0   1  \n",
       "464967  0.0  0.0   1  \n",
       "106241  0.0  0.0   0  \n",
       "43109   0.0  0.0   1  \n",
       "462810  0.0  0.0   0  \n",
       "289791  0.0  0.0   0  \n",
       "173926  0.0  0.0   1  \n",
       "233859  0.0  0.0   1  \n",
       "\n",
       "[10 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(covtype_df)\n",
    "covtype_df = pd.DataFrame(scaler.transform(covtype_df),columns=covtype_df.columns)\n",
    "covtype_df.insert(54,54,pred_column,True)\n",
    "covtype_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_col,pred_column = letter_df[0], letter_df[16]\n",
    "letter_df.drop(columns=[0,16],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7111</th>\n",
       "      <td>S</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>L</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>M</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>U</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8929</th>\n",
       "      <td>C</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>N</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411</th>\n",
       "      <td>N</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>W</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>V</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15948</th>\n",
       "      <td>B</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "7111   S  0.266667  0.400000  0.400000  0.266667  0.400000  0.600000   \n",
       "1167   L  0.200000  0.400000  0.200000  0.266667  0.066667  0.000000   \n",
       "5437   M  0.333333  0.533333  0.533333  0.400000  0.466667  0.466667   \n",
       "11917  U  0.600000  0.733333  0.666667  0.533333  0.466667  0.333333   \n",
       "8929   C  0.200000  0.466667  0.266667  0.333333  0.133333  0.200000   \n",
       "6328   N  0.266667  0.600000  0.333333  0.466667  0.266667  0.533333   \n",
       "9411   N  0.600000  0.800000  0.466667  0.466667  0.200000  0.600000   \n",
       "8289   W  0.333333  0.266667  0.400000  0.200000  0.200000  0.333333   \n",
       "3440   V  0.133333  0.200000  0.266667  0.266667  0.066667  0.466667   \n",
       "15948  B  0.133333  0.266667  0.266667  0.133333  0.200000  0.600000   \n",
       "\n",
       "             7         8         9         10        11        12        13  \\\n",
       "7111   0.400000  0.266667  0.200000  0.600000  0.400000  0.600000  0.266667   \n",
       "1167   0.066667  0.400000  0.400000  0.000000  0.066667  0.333333  0.000000   \n",
       "5437   0.400000  0.333333  0.333333  0.466667  0.466667  0.733333  0.933333   \n",
       "11917  0.466667  0.333333  0.600000  0.600000  0.400000  0.600000  0.533333   \n",
       "8929   0.600000  0.333333  0.533333  0.733333  0.733333  0.733333  0.066667   \n",
       "6328   0.466667  0.866667  0.066667  0.400000  0.400000  0.533333  0.333333   \n",
       "9411   0.733333  0.333333  0.400000  0.200000  0.333333  0.600000  0.333333   \n",
       "8289   0.733333  0.200000  0.133333  0.600000  0.533333  0.466667  0.466667   \n",
       "3440   0.533333  0.266667  0.133333  0.466667  0.866667  0.533333  0.200000   \n",
       "15948  0.466667  0.133333  0.400000  0.733333  0.333333  0.466667  0.266667   \n",
       "\n",
       "             14        15  16  17  18  \n",
       "7111   0.533333  0.666667  10   0   0  \n",
       "1167   0.533333  0.000000   8   0   1  \n",
       "5437   0.400000  0.133333  10   0   1  \n",
       "11917  0.600000  0.400000   1   0   0  \n",
       "8929   0.533333  0.200000   7   0   1  \n",
       "6328   0.600000  0.000000   8   0   0  \n",
       "9411   0.533333  0.133333   7   0   0  \n",
       "8289   0.800000  0.066667   6   0   0  \n",
       "3440   0.666667  0.000000   8   0   0  \n",
       "15948  0.466667  0.333333   9   0   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(letter_df)\n",
    "letter_df = pd.DataFrame(scaler.transform(letter_df),columns=letter_df.columns)\n",
    "letter_df.insert(0,0,letter_col,True)\n",
    "letter_df.insert(16,16,pred_column,True)\n",
    "\n",
    "# two ways to make binary classification labels\n",
    "letter_df_p1 = np.where(letter_df[0] == 'O', 1, 0)\n",
    "letter_df_p2 = np.where(letter_df[0].isin([chr(x) for x in range(ord('A'), ord('M') + 1)]), 1, 0)\n",
    "letter_df.insert(17,17,letter_df_p1,True)\n",
    "letter_df.insert(18,18,letter_df_p2,True)\n",
    "\n",
    "\n",
    "letter_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "search_space = [{'classifier': [LogisticRegression(solver='saga')],\n",
    "                 'classifier__penalty': ['none','l1','l2'],\n",
    "                 'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 100, 1000],\n",
    "                 'classifier__max_features': [1,2,6,12, 20]}]\n",
    "# Create grid search \n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), verbose=0)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(covtype_df.iloc[:5000,:-1], covtype_df.iloc[:5000,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = covtype_df.iloc[:10000,:-1], covtype_df.iloc[:10000,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# take all our penguin data, and reserve 50% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(solver='saga',\n",
    "                          random_state=0)\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = SVC(random_state=0)\n",
    "\n",
    "clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "clf5 = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "# clf6 = OrthogonalMatchingPursuit()\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('classifier', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('classifier', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('classifier', clf5)])\n",
    "\n",
    "# pipe6 = Pipeline([('classifier', clf6)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__penalty': ['none', 'l1', 'l2'],\n",
    "                'classifier__C': np.logspace(-8, 4, 13)}]\n",
    "\n",
    "param_grid2 = [{'classifier__n_neighbors': np.arange(1, 100, 10)}]\n",
    "#                 'classifier__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'classifier__kernel': ['rbf'],\n",
    "                'classifier__C': np.power(10., np.arange(-4, 4)),\n",
    "                'classifier__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'classifier__kernel': ['linear'],                \n",
    "                'classifier__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid4 = [{'classifier__n_estimators': [1024],\n",
    "                'classifier__max_features': [1,2,4,6,8,12,16,20]}]\n",
    "\n",
    "param_grid5 = [{'classifier__C': np.logspace(-8,4,13),\n",
    "               'classifier__loss': ['hinge', 'squared_hinge']}]\n",
    "\n",
    "# param_grid6 = [{'classifier__n_nonzero_coefs': [.01, .05, .1, .2, .6]}]\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                            (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                            ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=1,\n",
    "                       cv=2, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "?accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 88.85% | outer ACC 88.70%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 87.78% | outer ACC 86.80%\n",
      "outer fold 1/5 | tuning PAC      | inner ACC 87.15% | outer ACC 86.50%\n",
      "outer fold 1/5 | tuning RF       | inner ACC 90.62% | outer ACC 90.70%\n",
      "outer fold 1/5 | tuning SVM      | inner ACC 88.70% | outer ACC 88.20%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 88.15% | outer ACC 88.90%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 87.48% | outer ACC 87.40%\n",
      "outer fold 2/5 | tuning PAC      | inner ACC 87.13% | outer ACC 87.30%\n",
      "outer fold 2/5 | tuning RF       | inner ACC 90.72% | outer ACC 90.70%\n",
      "outer fold 2/5 | tuning SVM      | inner ACC 88.95% | outer ACC 89.30%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 88.22% | outer ACC 88.80%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 87.28% | outer ACC 88.00%\n",
      "outer fold 3/5 | tuning PAC      | inner ACC 87.05% | outer ACC 87.30%\n",
      "outer fold 3/5 | tuning RF       | inner ACC 89.85% | outer ACC 92.10%\n",
      "outer fold 3/5 | tuning SVM      | inner ACC 88.40% | outer ACC 88.70%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 88.15% | outer ACC 90.00%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 87.40% | outer ACC 87.20%\n",
      "outer fold 4/5 | tuning PAC      | inner ACC 87.20% | outer ACC 87.10%\n",
      "outer fold 4/5 | tuning RF       | inner ACC 90.40% | outer ACC 90.90%\n",
      "outer fold 4/5 | tuning SVM      | inner ACC 88.42% | outer ACC 89.10%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 88.38% | outer ACC 89.20%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 87.40% | outer ACC 87.60%\n",
      "outer fold 5/5 | tuning PAC      | inner ACC 87.33% | outer ACC 86.80%\n",
      "outer fold 5/5 | tuning RF       | inner ACC 90.62% | outer ACC 90.60%\n",
      "outer fold 5/5 | tuning SVM      | inner ACC 88.83% | outer ACC 89.40%\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the \n",
    "    # penguin data is in a Dataframe instead of a numpy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic | outer CV acc. 87.40% +\\- 0.400\n",
      "KNN      | outer CV acc. 89.12% +\\- 0.471\n",
      "SVM      | outer CV acc. 88.94% +\\- 0.441\n",
      "RF       | outer CV acc. 91.00% +\\- 0.559\n",
      "PAC      | outer CV acc. 87.00% +\\- 0.310\n",
      "\n",
      "Logistic best parameters {'classifier__C': 100.0, 'classifier__penalty': 'l1'}\n",
      "KNN best parameters {'classifier__n_neighbors': 11}\n",
      "SVM best parameters {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "RF best parameters {'classifier__max_features': 20, 'classifier__n_estimators': 1024}\n",
      "PAC best parameters {'classifier__C': 0.1, 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 88.62% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['SVM'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = letter_df.iloc[:10000,1:-2], letter_df.iloc[:10000,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# take all our penguin data, and reserve 50% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(solver='saga',\n",
    "                          random_state=0)\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = SVC(random_state=0)\n",
    "\n",
    "clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "clf5 = PassiveAggressiveClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "\n",
    "# clf6 = OrthogonalMatchingPursuit()\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('classifier', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('classifier', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('classifier', clf5)])\n",
    "\n",
    "# pipe6 = Pipeline([('classifier', clf6)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__penalty': ['none', 'l1', 'l2'],\n",
    "                'classifier__C': np.logspace(-8, 4, 13)}]\n",
    "\n",
    "param_grid2 = [{'classifier__n_neighbors': np.arange(1, 100, 10)}]\n",
    "#                 'classifier__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'classifier__kernel': ['rbf'],\n",
    "                'classifier__C': np.power(10., np.arange(-4, 4)),\n",
    "                'classifier__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'classifier__kernel': ['linear'],                \n",
    "                'classifier__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid4 = [{'classifier__n_estimators': [1024],\n",
    "                'classifier__max_features': [1,2,4,6,8,12,16,20]}]\n",
    "\n",
    "param_grid5 = [{'classifier__C': np.logspace(-8,4,13),\n",
    "               'classifier__loss': ['hinge', 'squared_hinge']}]\n",
    "\n",
    "# param_grid6 = [{'classifier__n_nonzero_coefs': [.01, .05, .1, .2, .6]}]\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,param_grid5),\n",
    "                            (pipe1, pipe2, pipe3, pipe4, pipe5),\n",
    "                            ('Logistic', 'KNN', 'SVM', 'RF', 'PAC')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=1,\n",
    "                       cv=2, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 97.78% | outer ACC 98.60%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 1/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 1/5 | tuning RF       | inner ACC 98.38% | outer ACC 98.90%\n",
      "outer fold 1/5 | tuning SVM      | inner ACC 97.97% | outer ACC 98.60%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 98.23% | outer ACC 98.60%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 2/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 2/5 | tuning RF       | inner ACC 98.40% | outer ACC 98.50%\n",
      "outer fold 2/5 | tuning SVM      | inner ACC 98.50% | outer ACC 98.10%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 97.85% | outer ACC 98.30%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 3/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 3/5 | tuning RF       | inner ACC 98.38% | outer ACC 98.30%\n",
      "outer fold 3/5 | tuning SVM      | inner ACC 98.12% | outer ACC 98.70%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 97.88% | outer ACC 98.80%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 4/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 4/5 | tuning RF       | inner ACC 98.23% | outer ACC 98.70%\n",
      "outer fold 4/5 | tuning SVM      | inner ACC 98.20% | outer ACC 98.50%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 97.88% | outer ACC 99.10%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 5/5 | tuning PAC      | inner ACC 96.20% | outer ACC 96.20%\n",
      "outer fold 5/5 | tuning RF       | inner ACC 98.28% | outer ACC 98.40%\n",
      "outer fold 5/5 | tuning SVM      | inner ACC 98.12% | outer ACC 98.50%\n",
      "Wall time: 8h 45min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    #FYI: This code uses X_train.iloc[... ] instead of X_train[...] because the \n",
    "    # penguin data is in a Dataframe instead of a numpy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic | outer CV acc. 96.20% +\\- 0.000\n",
      "KNN      | outer CV acc. 98.68% +\\- 0.264\n",
      "SVM      | outer CV acc. 98.48% +\\- 0.204\n",
      "RF       | outer CV acc. 98.56% +\\- 0.215\n",
      "PAC      | outer CV acc. 96.20% +\\- 0.000\n",
      "\n",
      "Logistic best parameters {'classifier__C': 1e-08, 'classifier__penalty': 'none'}\n",
      "KNN best parameters {'classifier__n_neighbors': 1}\n",
      "SVM best parameters {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "RF best parameters {'classifier__max_features': 6, 'classifier__n_estimators': 1024}\n",
      "PAC best parameters {'classifier__C': 1e-08, 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 98.14% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 98.58%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['SVM'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00999248, 0.00897574, 0.00997722, 0.00846028, 0.01097262,\n",
       "        0.01098752, 0.00797904, 0.00797808, 0.01346684, 0.00997305]),\n",
       " 'std_fit_time': array([1.98352337e-03, 9.96589661e-04, 1.54972076e-06, 5.14507294e-04,\n",
       "        1.99687481e-03, 9.82999802e-04, 9.96708870e-04, 1.19209290e-07,\n",
       "        3.46350670e-03, 9.97543335e-04]),\n",
       " 'mean_score_time': array([0.16227698, 0.26413703, 0.28124559, 0.25383055, 0.24334741,\n",
       "        0.2228924 , 0.30595279, 0.27277231, 0.36454248, 0.28488588]),\n",
       " 'std_score_time': array([0.0022912 , 0.00365424, 0.04188406, 0.01446545, 0.01396132,\n",
       "        0.00147712, 0.05162477, 0.00349283, 0.03889847, 0.01658368]),\n",
       " 'param_classifier__n_neighbors': masked_array(data=[1, 11, 21, 31, 41, 51, 61, 71, 81, 91],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__n_neighbors': 1},\n",
       "  {'classifier__n_neighbors': 11},\n",
       "  {'classifier__n_neighbors': 21},\n",
       "  {'classifier__n_neighbors': 31},\n",
       "  {'classifier__n_neighbors': 41},\n",
       "  {'classifier__n_neighbors': 51},\n",
       "  {'classifier__n_neighbors': 61},\n",
       "  {'classifier__n_neighbors': 71},\n",
       "  {'classifier__n_neighbors': 81},\n",
       "  {'classifier__n_neighbors': 91}],\n",
       " 'split0_test_score': array([0.982 , 0.9796, 0.9772, 0.9768, 0.9724, 0.9684, 0.9656, 0.9628,\n",
       "        0.9624, 0.962 ]),\n",
       " 'split1_test_score': array([0.9808, 0.9756, 0.9784, 0.9732, 0.9712, 0.9692, 0.964 , 0.962 ,\n",
       "        0.962 , 0.962 ]),\n",
       " 'mean_test_score': array([0.9814, 0.9776, 0.9778, 0.975 , 0.9718, 0.9688, 0.9648, 0.9624,\n",
       "        0.9622, 0.962 ]),\n",
       " 'std_test_score': array([0.0006, 0.002 , 0.0006, 0.0018, 0.0006, 0.0004, 0.0008, 0.0004,\n",
       "        0.0002, 0.    ]),\n",
       " 'rank_test_score': array([ 1,  3,  2,  4,  5,  6,  7,  8,  9, 10])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_algo.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([covtype_df.iloc[5000:,:-1], covtype_df.iloc[5000:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = best_model.best_params_['classifier'].fit(covtype_df.iloc[:5000,:-1], covtype_df.iloc[:5000,-1]).predict(covtype_df.iloc[5000:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-38401116fb0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovtype_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovtype_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-700440aa295b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovtype_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters = {'C':[1],'penalty':['l2']}\n",
    "X = covtype_df.drop(columns=[0])\n",
    "y = np.where(covtype_df[16] == 2,1,0)\n",
    "log_reg = LogisticRegression()\n",
    "clf = GridSearchCV(estimator=log_reg,param_grid=parameters)\n",
    "clf.fit(X,y)\n",
    "print(clf.score(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = adult_df[['workclass', 'marital-status', 'occupation', 'relationship']]\n",
    "y = np.where(adult_df.iloc[:,-1] == ' >50K',1,0)\n",
    "\n",
    "column_trans = make_column_transformer((OneHotEncoder(handle_unknown='ignore'),\n",
    "                                        ['workclass', 'marital-status', 'occupation']),\n",
    "                                      (OrdinalEncoder(), ['relationship']),\n",
    "                                      remainder='drop')\n",
    "logreg = LogisticRegression()\n",
    "pipe = make_pipeline(column_trans, logreg)\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='f1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
